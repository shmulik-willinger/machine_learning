{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_input = np.array([[1,2,3,4,5]])\n",
    "y_input = np.array([[10]])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 5])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "print(x_input)\n",
    "print(y_input)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([5, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y_pred = tf.matmul(x, W)+b\n",
    "\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a loss function\n",
    "loss = tf.reduce_sum(tf.pow((y-y_pred), 2))\n",
    "\n",
    "#specify the optimizer and the variable that we want to minimize\n",
    "train = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "# initialize all variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(10):\n",
    "    feed_dict = {x: x_input, y: y_input}\n",
    "    _, loss_value = sess.run([train, loss], feed_dict=feed_dict)\n",
    "    print(loss_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_input = np.array([[1,2,3,4,5]])\n",
    "y_input = np.array([[10]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=32, input_dim=x_input.shape[1]))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_input, y_input, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_input, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# We will be using the Iris Plants Database\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first two classes (Iris-Setosa and Iris-Versicolour) are linear separable\n",
    "iris = load_iris()\n",
    "idxs = np.where(iris.target<2)\n",
    "X = iris.data[idxs]\n",
    "Y = iris.target[idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[Y==0][:,0],X[Y==0][:,2], color='green', label='Iris-Setosa')\n",
    "plt.scatter(X[Y==1][:,0],X[Y==1][:,2], color='red', label='Iris-Versicolour')\n",
    "plt.title('Iris Plants Database')\n",
    "plt.xlabel('sepal length in cm')\n",
    "plt.ylabel('sepal width in cm')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "weights = np.random.normal(size=X_train.shape[1])\n",
    "bias = 1\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_epochs = 15\n",
    "\n",
    "\n",
    "del_w = np.zeros(weights.shape)\n",
    "hist_loss = []\n",
    "hist_accuracy = []\n",
    "for i in range(n_epochs):\n",
    "    # We apply a simple step function, if the output is > 0.5 we predict 1, else 0\n",
    "    output = np.where((X_train.dot(weights)+bias)>0.5, 1, 0)\n",
    "    \n",
    "    # Compute MSE\n",
    "    error = np.mean((y_train-output)**2)\n",
    " \n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * np.dot((output-y_train), X_train)\n",
    "    bias += learning_rate * np.sum(np.dot((output-y_train), X_train))\n",
    "    \n",
    "    # Calculate MSE\n",
    "    loss = np.mean((output - y_train) ** 2)\n",
    "    hist_loss.append(loss)\n",
    "    \n",
    "    # Determine validation accuracy\n",
    "    output_val = np.where(X_val.dot(weights)>0.5, 1, 0)\n",
    "    accuracy = np.mean(np.where(y_val==output_val, 1, 0))\n",
    "    hist_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "a = fig.add_subplot(1,2,1)\n",
    "imgplot = plt.plot(hist_loss)\n",
    "plt.xlabel('epochs')\n",
    "a.set_title('Training loss')\n",
    "\n",
    "\n",
    "a=fig.add_subplot(1,2,2)\n",
    "imgplot = plt.plot(hist_accuracy)\n",
    "plt.xlabel('epochs')\n",
    "a.set_title('Validation Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# We will be using make_circles from scikit-learn\n",
    "from sklearn.datasets import make_circles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create an inner and outer circle\n",
    "X, y = make_circles(n_samples=500, factor=.3, noise=.05, random_state=2017)\n",
    "outer = y == 0\n",
    "inner = y == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Two Circles\")\n",
    "plt.plot(X[outer, 0], X[outer, 1], \"ro\")\n",
    "plt.plot(X[inner, 0], X[inner, 1], \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We normalize the data to make sure the center of both circles is (1,1):\n",
    "X = X+1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# A linear activation function won't work in this case, so we'll be using a sigmoid function:\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# hyperparameters:\n",
    "n_hidden = 50 # number of hidden units\n",
    "n_epochs = 1000\n",
    "learning_rate = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the weights and other variables:\n",
    "\n",
    "weights_hidden = np.random.normal(0.0, size=(X_train.shape[1], n_hidden))\n",
    "weights_output = np.random.normal(0.0, size=(n_hidden))\n",
    "\n",
    "hist_loss = []\n",
    "hist_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e in range(n_epochs):\n",
    "    \n",
    "    del_w_hidden = np.zeros(weights_hidden.shape)\n",
    "    del_w_output = np.zeros(weights_output.shape)\n",
    "    \n",
    "    # Loop through training data in batches of 1\n",
    "    for x_, y_ in zip(X_train, y_train):\n",
    "        \n",
    "        # Forward computations\n",
    "        hidden_input = np.dot(x_, weights_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_output, weights_output))\n",
    "        \n",
    "        # Backward computations\n",
    "        error = y_ - output\n",
    "        output_error = error * output * (1 - output)\n",
    "        hidden_error = np.dot(output_error, weights_output) * hidden_output * (1 - hidden_output)\n",
    "        del_w_output += output_error * hidden_output\n",
    "        del_w_hidden += hidden_error * x_[:, None]\n",
    "        \n",
    "        # Update weights\n",
    "        weights_hidden += learning_rate * del_w_hidden / X_train.shape[0]\n",
    "        weights_output += learning_rate * del_w_output / X_train.shape[0]\n",
    "        \n",
    "    # Print stats (validation loss and accuracy)\n",
    "    if e % 100 == 0:\n",
    "        hidden_output = sigmoid(np.dot(X_val, weights_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output, weights_output))\n",
    "        loss = np.mean((out - y_val) ** 2)\n",
    "        # Final prediction is based on a threshold of 0.5\n",
    "        predictions = out > 0.5\n",
    "        accuracy = np.mean(predictions == y_val)\n",
    "        print(\"Epoch: \", '{:>4}'.format(e),\"; Validation loss: \", '{:>6}'.format(loss.round(4)), \"; Validation accuracy: \", '{:>6}'.format(accuracy.round(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('winequality-red.csv', sep=';')\n",
    "y = data['quality']\n",
    "X = data.drop(['quality'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Average quality training set: {:.4f}'.format(y_train.mean()))\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the input data:\n",
    "scalar = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(scalar.transform(X_train))\n",
    "X_test = pd.DataFrame(scalar.transform(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the mean quality of the training data for each validation input\n",
    "print('MSE:', np.mean((y_test - ([y_train.mean()] * y_test.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# callback for early stopping and saving the best model:\n",
    "callbacks= [EarlyStopping(monitor='val_acc', patience=20, verbose=1),\n",
    "            ModelCheckpoint('checkpoints/multi_layer_best_mode')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "n_epochs=5000\n",
    "model.fit(X_train.values, y_train, batch_size=batch_size, \n",
    "          epochs=n_epochs, verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = model\n",
    "best_model.load_weights('checkpoints/multi_layer_best_mode')\n",
    "best_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate on the test set\n",
    "score=best_model.evaluate(X_test.values, y_test, verbose=0)\n",
    "print('Test accuracy: %.2f%%' % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_val, y_val) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show an example of each label and print the count per label:\n",
    "unique_lables = set(y_train)\n",
    "plt.figure(figsize=(12,12))\n",
    "print(unique_lables)\n",
    "\n",
    "i=1\n",
    "for label in unique_lables:\n",
    "    image = X_train[y_train.tolist().index(label)]\n",
    "    plt.subplot(10,10,i)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{0}: ({1})\".format(label, y_train.tolist().count(label)))\n",
    "    i +=1\n",
    "    _ = plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save as JSON json_string = model.to_json()\n",
    "# save as YAML yaml_string = model.to_yaml()\n",
    "# model reconstruction from JSON: from keras.models import model_from_json model = model_from_json(json_string)\n",
    "\n",
    "from keras.models import load_model \n",
    "model.save('my_model.h5')\n",
    "load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import os\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 2\n",
    "MODEL_DIR = \"/tmp\"\n",
    "(Xtrain, ytrain), (Xtest, ytest) = mnist.load_data()\n",
    "Xtrain = Xtrain.reshape(60000, 784).astype(\"float32\") / 255\n",
    "Xtest = Xtest.reshape(10000, 784).astype(\"float32\") / 255\n",
    "Ytrain = np_utils.to_categorical(ytrain, 10)\n",
    "Ytest = np_utils.to_categorical(ytest, 10)\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,), activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# save best model\n",
    "checkpoint = ModelCheckpoint(filepath=os.path.join(MODEL_DIR, \"model-{epoch:02d}.h5\"),  save_best_only=True)\n",
    "model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, nb_epoch=NUM_EPOCHS, validation_split=0.1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from quiver_engine import server\n",
    "server.launch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 5\n",
    "NB_CLASSES = 10\n",
    "\n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#one-hot encoding and normalize the images\n",
    "X_train = X_train.astype('float32') /255\n",
    "X_test = X_test.astype('float32') /255\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=(32,32,3), activation='relu'))\n",
    "model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "40000/40000 [==============================] - 347s 9ms/step - loss: 1.7946 - acc: 0.3526 - val_loss: 1.3981 - val_acc: 0.5009\n",
      "Epoch 2/3\n",
      "40000/40000 [==============================] - 346s 9ms/step - loss: 1.3533 - acc: 0.5184 - val_loss: 1.1310 - val_acc: 0.5923\n",
      "Epoch 3/3\n",
      "40000/40000 [==============================] - 349s 9ms/step - loss: 1.1293 - acc: 0.5988 - val_loss: 0.9952 - val_acc: 0.6470\n",
      "10000/10000 [==============================] - 28s 3ms/step\n",
      "Test score:  0.996041975689\n",
      "Test accuracy:  0.6484\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "history_object = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=0.2, verbose=1)\n",
    "score= model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "print(\"Test score: \", score[0])\n",
    "print(\"Test accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWVIwxhgTZEnBGGNMkCUFY4wxQf8PjiEG3JS2dUUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1af86c4eeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['acc'])\n",
    "plt.title('Training results')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['error loss', 'accuracy'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training set images...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-087f55193e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# (3, 32, 32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (1, 3, 32, 32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mx_aug\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_to_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'preview'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cifar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'jpeg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_aug\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;31m# The transformation of images is not under thread lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[1;31m# so it can be done in parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mrandom_transform\u001b[1;34m(self, x, seed)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mtransform_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_matrix_offset_center\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             x = apply_transform(x, transform_matrix, img_channel_axis,\n\u001b[1;32m--> 634\u001b[1;33m                                 fill_mode=self.fill_mode, cval=self.cval)\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel_shift_range\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mapply_transform\u001b[1;34m(x, transform_matrix, channel_axis, fill_mode, cval)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         cval=cval) for x_channel in x]\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         cval=cval) for x_channel in x]\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\interpolation.py\u001b[0m in \u001b[0;36maffine_transform\u001b[1;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         _geometric_transform(filtered, None, None, matrix, offset,\n\u001b[1;32m--> 470\u001b[1;33m                              output, order, mode, cval, None, None)\n\u001b[0m\u001b[0;32m    471\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\interpolation.py\u001b[0m in \u001b[0;36m_geometric_transform\u001b[1;34m(input, mapping, coordinates, matrix, offset, output, order, mode, cval, extra_arguments, extra_keywords)\u001b[0m\n\u001b[0;32m    130\u001b[0m     _nd_image.geometric_transform(\n\u001b[0;32m    131\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         order, mode, cval, extra_arguments, extra_keywords)\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython.display import display \n",
    "from PIL import Image\n",
    "\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                              zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "xtas, ytas = [], []\n",
    "for i in range(X_train.shape[0]):\n",
    "    num_aug=0\n",
    "    x = X_train[i] # (3, 32, 32)\n",
    "    x = x.reshape((1,) + x.shape) # (1, 3, 32, 32)\n",
    "    for x_aug in datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='cifar', save_format='jpeg'):\n",
    "        if num_aug >= 5 :\n",
    "            break\n",
    "        xtas.append(x_aug[0])\n",
    "    num_aug += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = model_from_json(open('cifar10_architecture.json').read())\n",
    "model.load_weights('cifar10_weights.h5')\n",
    "\n",
    "img_names = ['preview/cifar_0_9794.jpeg', 'preview/cifar_0_9.jpeg']\n",
    "imgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), (32, 32)), (1, 0, 2)).astype('float32') for img_name in img_names]\n",
    "imgs = np.array(imgs) / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 65ms/step\n",
      "[3 6]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "predictions = model.predict_classes(imgs)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(1, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+ioAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26c79d02e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# prebuild model with pre-trained weights on imagenet\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "# resize into VGG16 trained images' format\n",
    "im = cv2.resize(cv2.imread('steam-locomotive.jpg'), (224, 224))  # (224, 224, 3)\n",
    "im = np.expand_dims(im, axis=0)  # (1, 224, 224, 3)\n",
    "\n",
    "# predict\n",
    "out = model.predict(im)\n",
    "plt.plot(out.ravel())\n",
    "plt.show()\n",
    "print (np.argmax(out))\n",
    "#this should print 820 for steaming train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
